{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing NLK and Checking Library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AbstractLazySequence',\n",
       " 'AffixTagger',\n",
       " 'AlignedSent',\n",
       " 'Alignment',\n",
       " 'AnnotationTask',\n",
       " 'ApplicationExpression',\n",
       " 'Assignment',\n",
       " 'BigramAssocMeasures',\n",
       " 'BigramCollocationFinder',\n",
       " 'BigramTagger',\n",
       " 'BinaryMaxentFeatureEncoding',\n",
       " 'BlanklineTokenizer',\n",
       " 'BllipParser',\n",
       " 'BottomUpChartParser',\n",
       " 'BottomUpLeftCornerChartParser',\n",
       " 'BottomUpProbabilisticChartParser',\n",
       " 'Boxer',\n",
       " 'BrillTagger',\n",
       " 'BrillTaggerTrainer',\n",
       " 'CFG',\n",
       " 'CRFTagger',\n",
       " 'CfgReadingCommand',\n",
       " 'ChartParser',\n",
       " 'ChunkParserI',\n",
       " 'ChunkScore',\n",
       " 'Cistem',\n",
       " 'ClassifierBasedPOSTagger',\n",
       " 'ClassifierBasedTagger',\n",
       " 'ClassifierI',\n",
       " 'ConcordanceIndex',\n",
       " 'ConditionalExponentialClassifier',\n",
       " 'ConditionalFreqDist',\n",
       " 'ConditionalProbDist',\n",
       " 'ConditionalProbDistI',\n",
       " 'ConfusionMatrix',\n",
       " 'ContextIndex',\n",
       " 'ContextTagger',\n",
       " 'ContingencyMeasures',\n",
       " 'CoreNLPDependencyParser',\n",
       " 'CoreNLPParser',\n",
       " 'Counter',\n",
       " 'CrossValidationProbDist',\n",
       " 'DRS',\n",
       " 'DecisionTreeClassifier',\n",
       " 'DefaultTagger',\n",
       " 'DependencyEvaluator',\n",
       " 'DependencyGrammar',\n",
       " 'DependencyGraph',\n",
       " 'DependencyProduction',\n",
       " 'DictionaryConditionalProbDist',\n",
       " 'DictionaryProbDist',\n",
       " 'DiscourseTester',\n",
       " 'DrtExpression',\n",
       " 'DrtGlueReadingCommand',\n",
       " 'ELEProbDist',\n",
       " 'EarleyChartParser',\n",
       " 'Expression',\n",
       " 'FStructure',\n",
       " 'FeatDict',\n",
       " 'FeatList',\n",
       " 'FeatStruct',\n",
       " 'FeatStructReader',\n",
       " 'Feature',\n",
       " 'FeatureBottomUpChartParser',\n",
       " 'FeatureBottomUpLeftCornerChartParser',\n",
       " 'FeatureChartParser',\n",
       " 'FeatureEarleyChartParser',\n",
       " 'FeatureIncrementalBottomUpChartParser',\n",
       " 'FeatureIncrementalBottomUpLeftCornerChartParser',\n",
       " 'FeatureIncrementalChartParser',\n",
       " 'FeatureIncrementalTopDownChartParser',\n",
       " 'FeatureTopDownChartParser',\n",
       " 'FreqDist',\n",
       " 'HTTPPasswordMgrWithDefaultRealm',\n",
       " 'HeldoutProbDist',\n",
       " 'HiddenMarkovModelTagger',\n",
       " 'HiddenMarkovModelTrainer',\n",
       " 'HunposTagger',\n",
       " 'IBMModel',\n",
       " 'IBMModel1',\n",
       " 'IBMModel2',\n",
       " 'IBMModel3',\n",
       " 'IBMModel4',\n",
       " 'IBMModel5',\n",
       " 'ISRIStemmer',\n",
       " 'ImmutableMultiParentedTree',\n",
       " 'ImmutableParentedTree',\n",
       " 'ImmutableProbabilisticMixIn',\n",
       " 'ImmutableProbabilisticTree',\n",
       " 'ImmutableTree',\n",
       " 'IncrementalBottomUpChartParser',\n",
       " 'IncrementalBottomUpLeftCornerChartParser',\n",
       " 'IncrementalChartParser',\n",
       " 'IncrementalLeftCornerChartParser',\n",
       " 'IncrementalTopDownChartParser',\n",
       " 'Index',\n",
       " 'InsideChartParser',\n",
       " 'JSONTaggedDecoder',\n",
       " 'JSONTaggedEncoder',\n",
       " 'KneserNeyProbDist',\n",
       " 'LancasterStemmer',\n",
       " 'LaplaceProbDist',\n",
       " 'LazyConcatenation',\n",
       " 'LazyEnumerate',\n",
       " 'LazyIteratorList',\n",
       " 'LazyMap',\n",
       " 'LazySubsequence',\n",
       " 'LazyZip',\n",
       " 'LeftCornerChartParser',\n",
       " 'LidstoneProbDist',\n",
       " 'LineTokenizer',\n",
       " 'LogicalExpressionException',\n",
       " 'LongestChartParser',\n",
       " 'MLEProbDist',\n",
       " 'MWETokenizer',\n",
       " 'Mace',\n",
       " 'MaceCommand',\n",
       " 'MaltParser',\n",
       " 'MaxentClassifier',\n",
       " 'Model',\n",
       " 'MultiClassifierI',\n",
       " 'MultiParentedTree',\n",
       " 'MutableProbDist',\n",
       " 'NLTKWordTokenizer',\n",
       " 'NaiveBayesClassifier',\n",
       " 'NaiveBayesDependencyScorer',\n",
       " 'NgramAssocMeasures',\n",
       " 'NgramTagger',\n",
       " 'NonprojectiveDependencyParser',\n",
       " 'Nonterminal',\n",
       " 'OrderedDict',\n",
       " 'PCFG',\n",
       " 'Paice',\n",
       " 'ParallelProverBuilder',\n",
       " 'ParallelProverBuilderCommand',\n",
       " 'ParentedTree',\n",
       " 'ParserI',\n",
       " 'PerceptronTagger',\n",
       " 'PhraseTable',\n",
       " 'PorterStemmer',\n",
       " 'PositiveNaiveBayesClassifier',\n",
       " 'ProbDistI',\n",
       " 'ProbabilisticDependencyGrammar',\n",
       " 'ProbabilisticMixIn',\n",
       " 'ProbabilisticNonprojectiveParser',\n",
       " 'ProbabilisticProduction',\n",
       " 'ProbabilisticProjectiveDependencyParser',\n",
       " 'ProbabilisticTree',\n",
       " 'Production',\n",
       " 'ProjectiveDependencyParser',\n",
       " 'Prover9',\n",
       " 'Prover9Command',\n",
       " 'ProxyBasicAuthHandler',\n",
       " 'ProxyDigestAuthHandler',\n",
       " 'ProxyHandler',\n",
       " 'PunktSentenceTokenizer',\n",
       " 'QuadgramAssocMeasures',\n",
       " 'QuadgramCollocationFinder',\n",
       " 'RSLPStemmer',\n",
       " 'RTEFeatureExtractor',\n",
       " 'RUS_PICKLE',\n",
       " 'RandomChartParser',\n",
       " 'RangeFeature',\n",
       " 'ReadingCommand',\n",
       " 'RecursiveDescentParser',\n",
       " 'RegexpChunkParser',\n",
       " 'RegexpParser',\n",
       " 'RegexpStemmer',\n",
       " 'RegexpTagger',\n",
       " 'RegexpTokenizer',\n",
       " 'ReppTokenizer',\n",
       " 'ResolutionProver',\n",
       " 'ResolutionProverCommand',\n",
       " 'SExprTokenizer',\n",
       " 'SLASH',\n",
       " 'Senna',\n",
       " 'SennaChunkTagger',\n",
       " 'SennaNERTagger',\n",
       " 'SennaTagger',\n",
       " 'SequentialBackoffTagger',\n",
       " 'ShiftReduceParser',\n",
       " 'SimpleGoodTuringProbDist',\n",
       " 'SklearnClassifier',\n",
       " 'SlashFeature',\n",
       " 'SnowballStemmer',\n",
       " 'SpaceTokenizer',\n",
       " 'StackDecoder',\n",
       " 'StanfordNERTagger',\n",
       " 'StanfordPOSTagger',\n",
       " 'StanfordSegmenter',\n",
       " 'StanfordTagger',\n",
       " 'StemmerI',\n",
       " 'SteppingChartParser',\n",
       " 'SteppingRecursiveDescentParser',\n",
       " 'SteppingShiftReduceParser',\n",
       " 'SyllableTokenizer',\n",
       " 'TYPE',\n",
       " 'TabTokenizer',\n",
       " 'TableauProver',\n",
       " 'TableauProverCommand',\n",
       " 'TaggerI',\n",
       " 'TestGrammar',\n",
       " 'Text',\n",
       " 'TextCat',\n",
       " 'TextCollection',\n",
       " 'TextTilingTokenizer',\n",
       " 'TnT',\n",
       " 'TokenSearcher',\n",
       " 'ToktokTokenizer',\n",
       " 'TopDownChartParser',\n",
       " 'TransitionParser',\n",
       " 'Tree',\n",
       " 'TreebankWordTokenizer',\n",
       " 'Trie',\n",
       " 'TrigramAssocMeasures',\n",
       " 'TrigramCollocationFinder',\n",
       " 'TrigramTagger',\n",
       " 'TweetTokenizer',\n",
       " 'TypedMaxentFeatureEncoding',\n",
       " 'Undefined',\n",
       " 'UniformProbDist',\n",
       " 'UnigramTagger',\n",
       " 'UnsortedChartParser',\n",
       " 'Valuation',\n",
       " 'Variable',\n",
       " 'ViterbiParser',\n",
       " 'WekaClassifier',\n",
       " 'WhitespaceTokenizer',\n",
       " 'WittenBellProbDist',\n",
       " 'WordNetLemmatizer',\n",
       " 'WordPunctTokenizer',\n",
       " '__author__',\n",
       " '__author_email__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__classifiers__',\n",
       " '__copyright__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__keywords__',\n",
       " '__license__',\n",
       " '__loader__',\n",
       " '__longdescr__',\n",
       " '__maintainer__',\n",
       " '__maintainer_email__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__url__',\n",
       " '__version__',\n",
       " 'accuracy',\n",
       " 'add_logs',\n",
       " 'agreement',\n",
       " 'align',\n",
       " 'alignment_error_rate',\n",
       " 'aline',\n",
       " 'api',\n",
       " 'app',\n",
       " 'apply_features',\n",
       " 'approxrand',\n",
       " 'arity',\n",
       " 'association',\n",
       " 'bigrams',\n",
       " 'binary_distance',\n",
       " 'binary_search_file',\n",
       " 'binding_ops',\n",
       " 'bisect',\n",
       " 'blankline_tokenize',\n",
       " 'bleu',\n",
       " 'bleu_score',\n",
       " 'bllip',\n",
       " 'boolean_ops',\n",
       " 'boxer',\n",
       " 'bracket_parse',\n",
       " 'breadth_first',\n",
       " 'brill',\n",
       " 'brill_trainer',\n",
       " 'build_opener',\n",
       " 'call_megam',\n",
       " 'casual',\n",
       " 'casual_tokenize',\n",
       " 'ccg',\n",
       " 'chain',\n",
       " 'chart',\n",
       " 'chat',\n",
       " 'choose',\n",
       " 'chunk',\n",
       " 'cistem',\n",
       " 'classify',\n",
       " 'clause',\n",
       " 'clean_html',\n",
       " 'clean_url',\n",
       " 'cluster',\n",
       " 'collections',\n",
       " 'collocations',\n",
       " 'combinations',\n",
       " 'compat',\n",
       " 'config_java',\n",
       " 'config_megam',\n",
       " 'config_weka',\n",
       " 'conflicts',\n",
       " 'confusionmatrix',\n",
       " 'conllstr2tree',\n",
       " 'conlltags2tree',\n",
       " 'corenlp',\n",
       " 'corpus',\n",
       " 'crf',\n",
       " 'custom_distance',\n",
       " 'data',\n",
       " 'decisiontree',\n",
       " 'decorator',\n",
       " 'decorators',\n",
       " 'defaultdict',\n",
       " 'demo',\n",
       " 'dependencygraph',\n",
       " 'deque',\n",
       " 'destructive',\n",
       " 'discourse',\n",
       " 'distance',\n",
       " 'download',\n",
       " 'download_gui',\n",
       " 'download_shell',\n",
       " 'downloader',\n",
       " 'draw',\n",
       " 'drt',\n",
       " 'earleychart',\n",
       " 'edit_distance',\n",
       " 'edit_distance_align',\n",
       " 'elementtree_indent',\n",
       " 'entropy',\n",
       " 'equality_preds',\n",
       " 'evaluate',\n",
       " 'evaluate_sents',\n",
       " 'everygrams',\n",
       " 'extract_rels',\n",
       " 'extract_test_sentences',\n",
       " 'f_measure',\n",
       " 'featstruct',\n",
       " 'featurechart',\n",
       " 'filestring',\n",
       " 'find',\n",
       " 'flatten',\n",
       " 'fractional_presence',\n",
       " 'getproxies',\n",
       " 'ghd',\n",
       " 'glue',\n",
       " 'grammar',\n",
       " 'guess_encoding',\n",
       " 'help',\n",
       " 'hmm',\n",
       " 'hunpos',\n",
       " 'ibm1',\n",
       " 'ibm2',\n",
       " 'ibm3',\n",
       " 'ibm4',\n",
       " 'ibm5',\n",
       " 'ibm_model',\n",
       " 'ieerstr2tree',\n",
       " 'in_idle',\n",
       " 'induce_pcfg',\n",
       " 'inference',\n",
       " 'infile',\n",
       " 'inspect',\n",
       " 'install_opener',\n",
       " 'internals',\n",
       " 'interpret_sents',\n",
       " 'interval_distance',\n",
       " 'invert_dict',\n",
       " 'invert_graph',\n",
       " 'is_rel',\n",
       " 'islice',\n",
       " 'isri',\n",
       " 'jaccard_distance',\n",
       " 'json_tags',\n",
       " 'jsontags',\n",
       " 'lancaster',\n",
       " 'lazyimport',\n",
       " 'lfg',\n",
       " 'line_tokenize',\n",
       " 'linearlogic',\n",
       " 'lm',\n",
       " 'load',\n",
       " 'load_parser',\n",
       " 'locale',\n",
       " 'log_likelihood',\n",
       " 'logic',\n",
       " 'mace',\n",
       " 'malt',\n",
       " 'map_tag',\n",
       " 'mapping',\n",
       " 'masi_distance',\n",
       " 'maxent',\n",
       " 'megam',\n",
       " 'memoize',\n",
       " 'meteor',\n",
       " 'meteor_score',\n",
       " 'metrics',\n",
       " 'misc',\n",
       " 'mwe',\n",
       " 'naivebayes',\n",
       " 'ne_chunk',\n",
       " 'ne_chunk_sents',\n",
       " 'ngrams',\n",
       " 'nonprojectivedependencyparser',\n",
       " 'nonterminals',\n",
       " 'numpy',\n",
       " 'os',\n",
       " 'pad_sequence',\n",
       " 'paice',\n",
       " 'pairwise',\n",
       " 'parallelize_preprocess',\n",
       " 'parse',\n",
       " 'parse_sents',\n",
       " 'pchart',\n",
       " 'perceptron',\n",
       " 'pk',\n",
       " 'porter',\n",
       " 'pos_tag',\n",
       " 'pos_tag_sents',\n",
       " 'positivenaivebayes',\n",
       " 'pprint',\n",
       " 'pr',\n",
       " 'precision',\n",
       " 'presence',\n",
       " 'print_string',\n",
       " 'probability',\n",
       " 'projectivedependencyparser',\n",
       " 'prover9',\n",
       " 'punkt',\n",
       " 'py25',\n",
       " 'py26',\n",
       " 'py27',\n",
       " 'pydoc',\n",
       " 'raise_unorderable_types',\n",
       " 'ranks_from_scores',\n",
       " 'ranks_from_sequence',\n",
       " 're',\n",
       " 're_show',\n",
       " 'read_grammar',\n",
       " 'read_logic',\n",
       " 'read_valuation',\n",
       " 'recall',\n",
       " 'recursivedescent',\n",
       " 'regexp',\n",
       " 'regexp_span_tokenize',\n",
       " 'regexp_tokenize',\n",
       " 'register_tag',\n",
       " 'relextract',\n",
       " 'repp',\n",
       " 'resolution',\n",
       " 'ribes',\n",
       " 'ribes_score',\n",
       " 'root_semrep',\n",
       " 'rslp',\n",
       " 'rte_classifier',\n",
       " 'rte_classify',\n",
       " 'rte_features',\n",
       " 'rtuple',\n",
       " 'scikitlearn',\n",
       " 'scores',\n",
       " 'segmentation',\n",
       " 'sem',\n",
       " 'senna',\n",
       " 'sent_tokenize',\n",
       " 'sequential',\n",
       " 'set2rel',\n",
       " 'set_proxy',\n",
       " 'sexpr',\n",
       " 'sexpr_tokenize',\n",
       " 'shiftreduce',\n",
       " 'simple',\n",
       " 'sinica_parse',\n",
       " 'skipgrams',\n",
       " 'skolemize',\n",
       " 'slice_bounds',\n",
       " 'snowball',\n",
       " 'sonority_sequencing',\n",
       " 'spearman',\n",
       " 'spearman_correlation',\n",
       " 'stack_decoder',\n",
       " 'stanford',\n",
       " 'stanford_segmenter',\n",
       " 'stem',\n",
       " 'str2tuple',\n",
       " 'string_span_tokenize',\n",
       " 'subprocess',\n",
       " 'subsumes',\n",
       " 'sum_logs',\n",
       " 'sys',\n",
       " 'tableau',\n",
       " 'tadm',\n",
       " 'tag',\n",
       " 'tagset_mapping',\n",
       " 'tagstr2tree',\n",
       " 'tbl',\n",
       " 'tee',\n",
       " 'text',\n",
       " 'textcat',\n",
       " 'texttiling',\n",
       " 'textwrap',\n",
       " 'tkinter',\n",
       " 'tnt',\n",
       " 'tokenize',\n",
       " 'tokenwrap',\n",
       " 'toktok',\n",
       " 'toolbox',\n",
       " 'total_ordering',\n",
       " 'transitionparser',\n",
       " 'transitive_closure',\n",
       " 'translate',\n",
       " 'tree',\n",
       " 'tree2conllstr',\n",
       " 'tree2conlltags',\n",
       " 'treebank',\n",
       " 'treetransforms',\n",
       " 'trigrams',\n",
       " 'tuple2str',\n",
       " 'types',\n",
       " 'unify',\n",
       " 'unique_list',\n",
       " 'untag',\n",
       " 'usage',\n",
       " 'util',\n",
       " 'version_file',\n",
       " 'version_info',\n",
       " 'viterbi',\n",
       " 'weka',\n",
       " 'windowdiff',\n",
       " 'word_tokenize',\n",
       " 'wordnet',\n",
       " 'wordpunct_tokenize',\n",
       " 'wsd']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'herself', 'been', 'with', 'here', 'very', 'doesn', 'won']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')[:500:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Unstructured Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawData = open('SMSSpamCollection.tsv').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ham\\tI've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\\nspam\\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\\nham\\tNah I don't think he goes to usf, he lives around here though\\nham\\tEven my brother is not like to speak with me. They treat me like aid\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawData[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedData = rawData.replace('\\t', '\\n').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ham',\n",
       " \"I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\",\n",
       " 'spam',\n",
       " \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n",
       " 'ham']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsedData[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelList = parsedData[0::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "textList = parsedData[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham', 'spam', 'ham', 'ham', 'ham']\n"
     ]
    }
   ],
   "source": [
    "print(labelList[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\", \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\", \"Nah I don't think he goes to usf, he lives around here though\", 'Even my brother is not like to speak with me. They treat me like aids patent.', 'I HAVE A DATE ON SUNDAY WITH WILL!!']\n"
     ]
    }
   ],
   "source": [
    "print(textList[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5571\n"
     ]
    }
   ],
   "source": [
    "print(len(labelList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5570\n"
     ]
    }
   ],
   "source": [
    "print(len(textList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                          body_list\n",
       "0   ham  I've been searching for the right words to tha...\n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "3   ham  Even my brother is not like to speak with me. ...\n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullCorpus = pd.DataFrame({'label':labelList[:-1], \n",
    "                          'body_list':textList\n",
    "                          })\n",
    "fullCorpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullCorpus = pd.read_csv('SMSSpamCollection.tsv', \n",
    "                      sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                          body_text\n",
       "0   ham  I've been searching for the right words to tha...\n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "3   ham  Even my brother is not like to speak with me. ...\n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullCorpus.columns=['label', 'body_text']\n",
    "fullCorpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data has 5568 rows and  2 columns\n"
     ]
    }
   ],
   "source": [
    "print('Input data has {} rows and  {} columns'.format(len(fullCorpus), \n",
    "      len(fullCorpus.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 5568 rows, 746 are spam and 4822 are ham\n"
     ]
    }
   ],
   "source": [
    "# How many spam/ham are there?\n",
    "print(\"Out of {} rows, {} are spam and {} are ham\".format(\n",
    "    len(fullCorpus), \n",
    "    len(fullCorpus[fullCorpus['label']=='spam']), \n",
    "    len(fullCorpus[fullCorpus['label']=='ham'])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nulls in the label is 0\n",
      "Number of nulls in the body is  0\n"
     ]
    }
   ],
   "source": [
    "# How much missing data there is?\n",
    "print(\"Number of nulls in the label is {}\".format(fullCorpus['label'].isnull().sum()))\n",
    "      \n",
    "print(\"Number of nulls in the body is  {}\".format(fullCorpus['body_text'].isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions to Manipulate Text\n",
    "`nlp`, `[j-q]`, `[j-q]+`, `[0-9]+`, `[j-q0-09]+` ===> Usage `findall()` and `split()`\n",
    "- Used to identify whitespace between words/tokens\n",
    "- Create delimiters or end-of-line escape characters\n",
    "- Removing punctuation or numbers from the text\n",
    "- Cleaning HTML tags from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_test = 'This is a made up string to test 2 different regex methods'\n",
    "re_test_messy = 'This     is a made up     string to test 2    different regex methods'\n",
    "re_test_messy2 = 'This-is-a-made/up.string*to>>>>test----2\"\"\"\"\"different-regex-methods'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'made',\n",
       " 'up',\n",
       " 'string',\n",
       " 'to',\n",
       " 'test',\n",
       " '2',\n",
       " 'different',\n",
       " 'regex',\n",
       " 'methods']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\s', re_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'is',\n",
       " 'a',\n",
       " 'made',\n",
       " 'up',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'string',\n",
       " 'to',\n",
       " 'test',\n",
       " '2',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'different',\n",
       " 'regex',\n",
       " 'methods']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\s', re_test_messy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'made',\n",
       " 'up',\n",
       " 'string',\n",
       " 'to',\n",
       " 'test',\n",
       " '2',\n",
       " 'different',\n",
       " 'regex',\n",
       " 'methods']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\s+', re_test_messy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This-is-a-made/up.string*to>>>>test----2\"\"\"\"\"different-regex-methods']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\s+', re_test_messy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'made',\n",
       " 'up',\n",
       " 'string',\n",
       " 'to',\n",
       " 'test',\n",
       " '2',\n",
       " 'different',\n",
       " 'regex',\n",
       " 'methods']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\W+', re_test_messy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'made',\n",
       " 'up',\n",
       " 'string',\n",
       " 'to',\n",
       " 'test',\n",
       " '2',\n",
       " 'different',\n",
       " 'regex',\n",
       " 'methods']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\S+', re_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'made',\n",
       " 'up',\n",
       " 'string',\n",
       " 'to',\n",
       " 'test',\n",
       " '2',\n",
       " 'different',\n",
       " 'regex',\n",
       " 'methods']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\S+', re_test_messy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This-is-a-made/up.string*to>>>>test----2\"\"\"\"\"different-regex-methods']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\S+', re_test_messy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'made',\n",
       " 'up',\n",
       " 'string',\n",
       " 'to',\n",
       " 'test',\n",
       " '2',\n",
       " 'different',\n",
       " 'regex',\n",
       " 'methods']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\w+', re_test_messy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing Specific String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pep8_test = 'I try to follow PEP8 guidelines'\n",
    "pep7_test = 'I try to follow PEP7 guidelines'\n",
    "peep8_test = 'I try to follow PEEP8 guidelines'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PEEP8']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[A-Z]+[0-9]+', peep8_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I try to follow PEP8 Python Styleguide guidelines'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('[A-Z]+[0-9]+', 'PEP8 Python Styleguide', pep8_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I try to follow PEP8 Python Styleguide guidelines'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('[A-Z]+[0-9]+', 'PEP8 Python Styleguide', pep7_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I try to follow PEP8 Python Styleguide guidelines'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('[A-Z]+[0-9]+', 'PEP8 Python Styleguide', peep8_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing Data - Cleaning Text\n",
    "- **Remove punctuation**\n",
    "- **Tokenization**\n",
    "- **Remove stopwords**\n",
    "- Lemmatize/Stem (not in every NPL pipeline though), see Note 1.\n",
    "\n",
    "*Note 1: stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . If confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw depending on whether the use of the token was as a verb or a noun. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma. Linguistic processing for stemming or lemmatization is often done by an additional plug-in component to the indexing process, and a number of such components exist, both commercial and open-source.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...  \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "2                                        Nah I don't think he goes to usf, he lives around here though  \n",
       "3                        Even my brother is not like to speak with me. They treat me like aids patent.  \n",
       "4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "data = pd.read_csv('SMSSpamCollection.tsv', '\\t', header=None)\n",
    "data.columns = ['label', 'body_text']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_nostop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
       "      <td>['ive', 'searching', 'right', 'words', 'thank', 'breather', 'promise', 'wont', 'take', 'help', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>['free', 'entry', '2', 'wkly', 'comp', 'win', 'fa', 'cup', 'final', 'tkts', '21st', 'may', '2005...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>['nah', 'dont', 'think', 'goes', 'usf', 'lives', 'around', 'though']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>['even', 'brother', 'like', 'speak', 'treat', 'like', 'aids', 'patent']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>['date', 'sunday']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "2                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "3                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                                                                      body_text_nostop  \n",
       "0  ['ive', 'searching', 'right', 'words', 'thank', 'breather', 'promise', 'wont', 'take', 'help', '...  \n",
       "1  ['free', 'entry', '2', 'wkly', 'comp', 'win', 'fa', 'cup', 'final', 'tkts', '21st', 'may', '2005...  \n",
       "2                                 ['nah', 'dont', 'think', 'goes', 'usf', 'lives', 'around', 'though']  \n",
       "3                              ['even', 'brother', 'like', 'speak', 'treat', 'like', 'aids', 'patent']  \n",
       "4                                                                                   ['date', 'sunday']  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned = pd.read_csv('SMSSpamCollection_cleaned.tsv',\n",
    "                          sep='\\t')\n",
    "\n",
    "data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: revome punctuation\n",
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "   text_nopunct =  \"\".join([char for char in text if char not in string.punctuation])\n",
    "   return text_nopunct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
       "      <td>Ive been searching for the right words to thank you for this breather I promise i wont take your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>Even my brother is not like to speak with me They treat me like aids patent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "2                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "3                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                                                                       body_text_clean  \n",
       "0  Ive been searching for the right words to thank you for this breather I promise i wont take your...  \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...  \n",
       "2                                          Nah I dont think he goes to usf he lives around here though  \n",
       "3                          Even my brother is not like to speak with me They treat me like aids patent  \n",
       "4                                                                    I HAVE A DATE ON SUNDAY WITH WILL  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['body_text_clean'] = data['body_text'].apply(\n",
    "lambda x: remove_punct(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 : tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_clean</th>\n",
       "      <th>body_text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
       "      <td>Ive been searching for the right words to thank you for this breather I promise i wont take your...</td>\n",
       "      <td>[ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>Even my brother is not like to speak with me They treat me like aids patent</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "2                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "3                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                                                                       body_text_clean  \\\n",
       "0  Ive been searching for the right words to thank you for this breather I promise i wont take your...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...   \n",
       "2                                          Nah I dont think he goes to usf he lives around here though   \n",
       "3                          Even my brother is not like to speak with me They treat me like aids patent   \n",
       "4                                                                    I HAVE A DATE ON SUNDAY WITH WILL   \n",
       "\n",
       "                                                                                   body_text_tokenized  \n",
       "0  [ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...  \n",
       "1  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...  \n",
       "2                            [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]  \n",
       "3         [even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]  \n",
       "4                                                           [i, have, a, date, on, sunday, with, will]  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens\n",
    "\n",
    "data['body_text_tokenized'] = data['body_text_clean'].apply(lambda x: tokenize(x.lower()))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenized_list):\n",
    "    text =  [word for word in tokenized_list if word not in stopword]\n",
    "    return text\n",
    "\n",
    "data['body_text_nostop'] = data['body_text_tokenized'].apply(lambda x : remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_clean</th>\n",
       "      <th>body_text_tokenized</th>\n",
       "      <th>body_text_nostop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
       "      <td>Ive been searching for the right words to thank you for this breather I promise i wont take your...</td>\n",
       "      <td>[ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...</td>\n",
       "      <td>[ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, prom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>Even my brother is not like to speak with me They treat me like aids patent</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5563</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u. U have won the £750 Pound prize. 2 claim is easy...</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u U have won the £750 Pound prize 2 claim is easy c...</td>\n",
       "      <td>[this, is, the, 2nd, time, we, have, tried, 2, contact, u, u, have, won, the, 750, pound, prize,...</td>\n",
       "      <td>[2nd, time, tried, 2, contact, u, u, 750, pound, prize, 2, claim, easy, call, 087187272008, now1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5564</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "      <td>Will ü b going to esplanade fr home</td>\n",
       "      <td>[will, ü, b, going, to, esplanade, fr, home]</td>\n",
       "      <td>[ü, b, going, esplanade, fr, home]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5565</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other suggestions?</td>\n",
       "      <td>Pity  was in mood for that Soany other suggestions</td>\n",
       "      <td>[pity, was, in, mood, for, that, soany, other, suggestions]</td>\n",
       "      <td>[pity, mood, soany, suggestions]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd be interested in buying something else next week ...</td>\n",
       "      <td>The guy did some bitching but I acted like id be interested in buying something else next week a...</td>\n",
       "      <td>[the, guy, did, some, bitching, but, i, acted, like, id, be, interested, in, buying, something, ...</td>\n",
       "      <td>[guy, bitching, acted, like, id, interested, buying, something, else, next, week, gave, us, free]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>Rofl Its true to its name</td>\n",
       "      <td>[rofl, its, true, to, its, name]</td>\n",
       "      <td>[rofl, true, name]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5568 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  \\\n",
       "0      ham   \n",
       "1     spam   \n",
       "2      ham   \n",
       "3      ham   \n",
       "4      ham   \n",
       "...    ...   \n",
       "5563  spam   \n",
       "5564   ham   \n",
       "5565   ham   \n",
       "5566   ham   \n",
       "5567   ham   \n",
       "\n",
       "                                                                                                body_text  \\\n",
       "0     I've been searching for the right words to thank you for this breather. I promise i wont take yo...   \n",
       "1     Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "2                                           Nah I don't think he goes to usf, he lives around here though   \n",
       "3                           Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "4                                                                     I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "...                                                                                                   ...   \n",
       "5563  This is the 2nd time we have tried 2 contact u. U have won the £750 Pound prize. 2 claim is easy...   \n",
       "5564                                                                 Will ü b going to esplanade fr home?   \n",
       "5565                                            Pity, * was in mood for that. So...any other suggestions?   \n",
       "5566  The guy did some bitching but I acted like i'd be interested in buying something else next week ...   \n",
       "5567                                                                           Rofl. Its true to its name   \n",
       "\n",
       "                                                                                          body_text_clean  \\\n",
       "0     Ive been searching for the right words to thank you for this breather I promise i wont take your...   \n",
       "1     Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...   \n",
       "2                                             Nah I dont think he goes to usf he lives around here though   \n",
       "3                             Even my brother is not like to speak with me They treat me like aids patent   \n",
       "4                                                                       I HAVE A DATE ON SUNDAY WITH WILL   \n",
       "...                                                                                                   ...   \n",
       "5563  This is the 2nd time we have tried 2 contact u U have won the £750 Pound prize 2 claim is easy c...   \n",
       "5564                                                                  Will ü b going to esplanade fr home   \n",
       "5565                                                   Pity  was in mood for that Soany other suggestions   \n",
       "5566  The guy did some bitching but I acted like id be interested in buying something else next week a...   \n",
       "5567                                                                            Rofl Its true to its name   \n",
       "\n",
       "                                                                                      body_text_tokenized  \\\n",
       "0     [ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...   \n",
       "1     [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...   \n",
       "2                               [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]   \n",
       "3            [even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]   \n",
       "4                                                              [i, have, a, date, on, sunday, with, will]   \n",
       "...                                                                                                   ...   \n",
       "5563  [this, is, the, 2nd, time, we, have, tried, 2, contact, u, u, have, won, the, 750, pound, prize,...   \n",
       "5564                                                         [will, ü, b, going, to, esplanade, fr, home]   \n",
       "5565                                          [pity, was, in, mood, for, that, soany, other, suggestions]   \n",
       "5566  [the, guy, did, some, bitching, but, i, acted, like, id, be, interested, in, buying, something, ...   \n",
       "5567                                                                     [rofl, its, true, to, its, name]   \n",
       "\n",
       "                                                                                         body_text_nostop  \n",
       "0     [ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, prom...  \n",
       "1     [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...  \n",
       "2                                                    [nah, dont, think, goes, usf, lives, around, though]  \n",
       "3                                                 [even, brother, like, speak, treat, like, aids, patent]  \n",
       "4                                                                                          [date, sunday]  \n",
       "...                                                                                                   ...  \n",
       "5563  [2nd, time, tried, 2, contact, u, u, 750, pound, prize, 2, claim, easy, call, 087187272008, now1...  \n",
       "5564                                                                   [ü, b, going, esplanade, fr, home]  \n",
       "5565                                                                     [pity, mood, soany, suggestions]  \n",
       "5566    [guy, bitching, acted, like, id, interested, buying, something, else, next, week, gave, us, free]  \n",
       "5567                                                                                   [rofl, true, name]  \n",
       "\n",
       "[5568 rows x 5 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "- Reduces the corpus of words: Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MARTIN_EXTENSIONS',\n",
       " 'NLTK_EXTENSIONS',\n",
       " 'ORIGINAL_ALGORITHM',\n",
       " '__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_apply_rule_list',\n",
       " '_contains_vowel',\n",
       " '_ends_cvc',\n",
       " '_ends_double_consonant',\n",
       " '_has_positive_measure',\n",
       " '_is_consonant',\n",
       " '_measure',\n",
       " '_replace_suffix',\n",
       " '_step1a',\n",
       " '_step1b',\n",
       " '_step1c',\n",
       " '_step2',\n",
       " '_step3',\n",
       " '_step4',\n",
       " '_step5a',\n",
       " '_step5b',\n",
       " 'mode',\n",
       " 'pool',\n",
       " 'stem',\n",
       " 'vowels']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "ps = nltk.PorterStemmer()\n",
    "dir(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grow\n",
      "grow\n",
      "grow\n"
     ]
    }
   ],
   "source": [
    "print(ps.stem('grow'))\n",
    "print(ps.stem('growing'))\n",
    "print(ps.stem('grows'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though  \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.  \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!  \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep='\\t')\n",
    "data.columns = ['label', 'body_text']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [word for word in tokens if word not in stopwords]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['body_text_nostop'] = data['body_text'].apply(lambda x : clean_text(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_nostop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "      <td>[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...   \n",
       "\n",
       "                                                                                      body_text_nostop  \n",
       "0  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...  \n",
       "1                                                 [nah, dont, think, goes, usf, lives, around, though]  \n",
       "2                                              [even, brother, like, speak, treat, like, aids, patent]  \n",
       "3                                                                                       [date, sunday]  \n",
       "4  [per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_nostop</th>\n",
       "      <th>body_text_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "      <td>[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...</td>\n",
       "      <td>[per, request, mell, mell, oru, minnaminungint, nurungu, vettam, set, callertun, caller, press, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...   \n",
       "\n",
       "                                                                                      body_text_nostop  \\\n",
       "0  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...   \n",
       "1                                                 [nah, dont, think, goes, usf, lives, around, though]   \n",
       "2                                              [even, brother, like, speak, treat, like, aids, patent]   \n",
       "3                                                                                       [date, sunday]   \n",
       "4  [per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...   \n",
       "\n",
       "                                                                                     body_text_stemmed  \n",
       "0  [free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...  \n",
       "1                                                   [nah, dont, think, goe, usf, live, around, though]  \n",
       "2                                               [even, brother, like, speak, treat, like, aid, patent]  \n",
       "3                                                                                       [date, sunday]  \n",
       "4  [per, request, mell, mell, oru, minnaminungint, nurungu, vettam, set, callertun, caller, press, ...  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stemming(tokenized_text):\n",
    "    text = [ps.stem(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "\n",
    "data['body_text_stemmed'] = data['body_text_nostop'].apply(lambda x: stemming(x))\n",
    "data.head()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing\n",
    "- Tracking down to the synonyms with WorldNet Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though  \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.  \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!  \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep='\\t')\n",
    "data.columns = ['label', 'body_text']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_nostop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, final, tkts, 21st, May, 2005, Text, FA, 87121, receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[Nah, I, dont, think, goes, usf, lives, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>[Even, brother, like, speak, They, treat, like, aids, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>[I, HAVE, A, DATE, ON, SUNDAY, WITH, WILL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "      <td>[As, per, request, Melle, Melle, Oru, Minnaminunginte, Nurungu, Vettam, set, callertune, Callers...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...   \n",
       "\n",
       "                                                                                      body_text_nostop  \n",
       "0  [Free, entry, 2, wkly, comp, win, FA, Cup, final, tkts, 21st, May, 2005, Text, FA, 87121, receiv...  \n",
       "1                                              [Nah, I, dont, think, goes, usf, lives, around, though]  \n",
       "2                                        [Even, brother, like, speak, They, treat, like, aids, patent]  \n",
       "3                                                           [I, HAVE, A, DATE, ON, SUNDAY, WITH, WILL]  \n",
       "4  [As, per, request, Melle, Melle, Oru, Minnaminunginte, Nurungu, Vettam, set, callertune, Callers...  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['body_text_nostop'] = data['body_text'].apply(lambda x: clean_text(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_nostop</th>\n",
       "      <th>body_text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, final, tkts, 21st, May, 2005, Text, FA, 87121, receiv...</td>\n",
       "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, final, tkts, 21st, May, 2005, Text, FA, 87121, receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[Nah, I, dont, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[Nah, I, dont, think, go, usf, life, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>[Even, brother, like, speak, They, treat, like, aids, patent]</td>\n",
       "      <td>[Even, brother, like, speak, They, treat, like, aid, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>[I, HAVE, A, DATE, ON, SUNDAY, WITH, WILL]</td>\n",
       "      <td>[I, HAVE, A, DATE, ON, SUNDAY, WITH, WILL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "      <td>[As, per, request, Melle, Melle, Oru, Minnaminunginte, Nurungu, Vettam, set, callertune, Callers...</td>\n",
       "      <td>[As, per, request, Melle, Melle, Oru, Minnaminunginte, Nurungu, Vettam, set, callertune, Callers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To c...</td>\n",
       "      <td>[WINNER, As, valued, network, customer, selected, receivea, 900, prize, reward, To, claim, call,...</td>\n",
       "      <td>[WINNER, As, valued, network, customer, selected, receivea, 900, prize, reward, To, claim, call,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with came...</td>\n",
       "      <td>[Had, mobile, 11, months, U, R, entitled, Update, latest, colour, mobiles, camera, Free, Call, T...</td>\n",
       "      <td>[Had, mobile, 11, month, U, R, entitled, Update, latest, colour, mobile, camera, Free, Call, The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried ...</td>\n",
       "      <td>[Im, gonna, home, soon, dont, want, talk, stuff, anymore, tonight, k, Ive, cried, enough, today]</td>\n",
       "      <td>[Im, gonna, home, soon, dont, want, talk, stuff, anymore, tonight, k, Ive, cried, enough, today]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>SIX chances to win CASH! From 100 to 20,000 pounds txt&gt; CSH11 and send to 87575. Cost 150p/day, ...</td>\n",
       "      <td>[SIX, chances, win, CASH, From, 100, 20000, pounds, txt, CSH11, send, 87575, Cost, 150pday, 6day...</td>\n",
       "      <td>[SIX, chance, win, CASH, From, 100, 20000, pound, txt, CSH11, send, 87575, Cost, 150pday, 6days,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM...</td>\n",
       "      <td>[URGENT, You, 1, week, FREE, membership, 100000, Prize, Jackpot, Txt, word, CLAIM, No, 81010, TC...</td>\n",
       "      <td>[URGENT, You, 1, week, FREE, membership, 100000, Prize, Jackpot, Txt, word, CLAIM, No, 81010, TC...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "5  spam   \n",
       "6  spam   \n",
       "7   ham   \n",
       "8  spam   \n",
       "9  spam   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...   \n",
       "5  WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To c...   \n",
       "6  Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with came...   \n",
       "7  I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried ...   \n",
       "8  SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, ...   \n",
       "9  URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM...   \n",
       "\n",
       "                                                                                      body_text_nostop  \\\n",
       "0  [Free, entry, 2, wkly, comp, win, FA, Cup, final, tkts, 21st, May, 2005, Text, FA, 87121, receiv...   \n",
       "1                                              [Nah, I, dont, think, goes, usf, lives, around, though]   \n",
       "2                                        [Even, brother, like, speak, They, treat, like, aids, patent]   \n",
       "3                                                           [I, HAVE, A, DATE, ON, SUNDAY, WITH, WILL]   \n",
       "4  [As, per, request, Melle, Melle, Oru, Minnaminunginte, Nurungu, Vettam, set, callertune, Callers...   \n",
       "5  [WINNER, As, valued, network, customer, selected, receivea, 900, prize, reward, To, claim, call,...   \n",
       "6  [Had, mobile, 11, months, U, R, entitled, Update, latest, colour, mobiles, camera, Free, Call, T...   \n",
       "7     [Im, gonna, home, soon, dont, want, talk, stuff, anymore, tonight, k, Ive, cried, enough, today]   \n",
       "8  [SIX, chances, win, CASH, From, 100, 20000, pounds, txt, CSH11, send, 87575, Cost, 150pday, 6day...   \n",
       "9  [URGENT, You, 1, week, FREE, membership, 100000, Prize, Jackpot, Txt, word, CLAIM, No, 81010, TC...   \n",
       "\n",
       "                                                                                  body_text_lemmatized  \n",
       "0  [Free, entry, 2, wkly, comp, win, FA, Cup, final, tkts, 21st, May, 2005, Text, FA, 87121, receiv...  \n",
       "1                                                 [Nah, I, dont, think, go, usf, life, around, though]  \n",
       "2                                         [Even, brother, like, speak, They, treat, like, aid, patent]  \n",
       "3                                                           [I, HAVE, A, DATE, ON, SUNDAY, WITH, WILL]  \n",
       "4  [As, per, request, Melle, Melle, Oru, Minnaminunginte, Nurungu, Vettam, set, callertune, Callers...  \n",
       "5  [WINNER, As, valued, network, customer, selected, receivea, 900, prize, reward, To, claim, call,...  \n",
       "6  [Had, mobile, 11, month, U, R, entitled, Update, latest, colour, mobile, camera, Free, Call, The...  \n",
       "7     [Im, gonna, home, soon, dont, want, talk, stuff, anymore, tonight, k, Ive, cried, enough, today]  \n",
       "8  [SIX, chance, win, CASH, From, 100, 20000, pound, txt, CSH11, send, 87575, Cost, 150pday, 6days,...  \n",
       "9  [URGENT, You, 1, week, FREE, membership, 100000, Prize, Jackpot, Txt, word, CLAIM, No, 81010, TC...  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatizing(tokenized_text):\n",
    "    text = [wn.lemmatize(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "data['body_text_lemmatized'] = data['body_text_nostop'].apply(\n",
    "lambda x: lemmatizing(x))\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-170-09fd0d2c7803>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#nlp_functions.clean_text()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text_test'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnlp_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3848\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-170-09fd0d2c7803>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#nlp_functions.clean_text()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text_test'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnlp_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/code/nlp/nlp_with_nltk/nlp_functions.py\u001b[0m in \u001b[0;36mclean_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/code/nlp/nlp_with_nltk/nlp_functions.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'string' is not defined"
     ]
    }
   ],
   "source": [
    "#nlp_functions.clean_text()\n",
    "\n",
    "data['clean_text_test'] = data['body_text'].apply(lambda x: nlp_functions.clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
